{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Weight-Sharing for Neural Network Compression - PyTorch Tutorial\n",
    "\n",
    "This tutorial demonstrates the **Soft Weight-Sharing** approach from Ullrich, Meeds & Welling (ICLR 2017) using PyTorch.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Soft weight-sharing learns a Gaussian mixture model as an empirical prior over network weights. The key idea is that weights naturally cluster together during training, allowing us to:\n",
    "1. Replace individual weights with their cluster center (quantization)\n",
    "2. Store only cluster centers (codebook) + assignments\n",
    "3. Achieve high compression with minimal accuracy loss\n",
    "\n",
    "## Three-Phase Approach\n",
    "\n",
    "1. **PART 1: Pretrain** - Train a standard network on MNIST\n",
    "2. **PART 2: Retrain** - Retrain with a learned Gaussian mixture prior that encourages clustering\n",
    "3. **PART 3: Post-process** - Quantize weights to mixture means and evaluate compression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:17:28.679916Z",
     "start_time": "2025-10-17T13:17:27.457755Z"
    }
   },
   "source": "import os\nimport json\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# Import our soft weight-sharing modules\nfrom sws.models import TutorialNet\nfrom sws.data import make_loaders\nfrom sws.prior import init_mixture, MixturePrior\nfrom sws.train import train_standard, retrain_soft_weight_sharing, evaluate\nfrom sws.compress import compression_report\nfrom sws.utils import collect_weight_params, set_seed, get_device\nfrom sws.viz import TrainingGifVisualizer\nfrom scripts.tutorial_helpers import (\n    plot_weight_scatter,\n    plot_weight_histogram,\n    plot_mixture_components,\n    plot_comparison_histograms\n)\n\n# Set random seed for reproducibility\nset_seed(42)\ndevice = get_device()\nprint(f\"Using device: {device}\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ATDL2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:17:28.696744Z",
     "start_time": "2025-10-17T13:17:28.682930Z"
    }
   },
   "source": [
    "# Load MNIST with batch size 128\n",
    "train_loader, test_loader, num_classes = make_loaders(\n",
    "    dataset=\"mnist\",\n",
    "    batch_size=128,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000\n",
      "Test samples: 10000\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: Pretrain Network\n",
    "\n",
    "We first train a standard convolutional neural network on MNIST:\n",
    "- **Architecture**: 2 convolutional layers + 2 fully-connected layers\n",
    "- **Parameters**: ~642,000 trainable weights\n",
    "- **Training**: Standard cross-entropy loss with Adam optimizer\n",
    "- **Expected accuracy**: ~98-99%"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:17:28.703944Z",
     "start_time": "2025-10-17T13:17:28.699443Z"
    }
   },
   "source": [
    "# Create the model\n",
    "model = TutorialNet(num_classes=num_classes).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "weight_params = sum(p.numel() for p in collect_weight_params(model))\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Weight parameters (to be compressed): {weight_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "TutorialNet(\n",
      "  (conv1): Conv2d(1, 25, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (conv2): Conv2d(25, 50, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (fc1): Linear(in_features=1250, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 642,460\n",
      "Weight parameters (to be compressed): 641,875\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:17:37.580594Z",
     "start_time": "2025-10-17T13:17:28.706132Z"
    }
   },
   "source": [
    "# Pretrain for 20 epochs\n",
    "pretrain_acc = train_standard(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    epochs=50,\n",
    "    lr=1e-3,\n",
    "    wd=0.0,\n",
    "    optim_name=\"adam\",\n",
    "    eval_every=5,\n",
    "    desc=\"pretrain\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Pretrained model accuracy: {pretrain_acc:.4f} ({pretrain_acc*100:.2f}%)\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[pretrain] epoch 1/50:   0%|          | 0/469 [00:00<?, ?it/s]/opt/anaconda3/envs/ATDL2/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Pretrain for 20 epochs\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m pretrain_acc = \u001B[43mtrain_standard\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwd\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptim_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43madam\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43meval_every\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpretrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     13\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m✓ Pretrained model accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrain_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrain_acc*\u001B[32m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m%)\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/University/UCPH/ATDL/torch-SWS/sws/train.py:89\u001B[39m, in \u001B[36mtrain_standard\u001B[39m\u001B[34m(model, train_loader, test_loader, device, epochs, lr, wd, optim_name, logger, eval_every, desc)\u001B[39m\n\u001B[32m     87\u001B[39m x, y = x.to(device), y.to(device)\n\u001B[32m     88\u001B[39m opt.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m89\u001B[39m loss = criterion(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m, y)\n\u001B[32m     90\u001B[39m loss.backward()\n\u001B[32m     91\u001B[39m opt.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ATDL2/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ATDL2/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/University/UCPH/ATDL/torch-SWS/sws/models.py:54\u001B[39m, in \u001B[36mTutorialNet.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m     53\u001B[39m     x = F.relu(\u001B[38;5;28mself\u001B[39m.conv1(x))\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m     x = F.relu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconv2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m     55\u001B[39m     x = x.view(x.size(\u001B[32m0\u001B[39m), -\u001B[32m1\u001B[39m)\n\u001B[32m     56\u001B[39m     x = F.relu(\u001B[38;5;28mself\u001B[39m.fc1(x))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ATDL2/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ATDL2/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ATDL2/lib/python3.11/site-packages/torch/nn/modules/conv.py:548\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    547\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m548\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ATDL2/lib/python3.11/site-packages/torch/nn/modules/conv.py:543\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    531\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    532\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    533\u001B[39m         F.pad(\n\u001B[32m    534\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    541\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    542\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m543\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    544\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    545\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the pretrained weights\n",
    "os.makedirs(\"tutorial_torch\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"tutorial_torch/pretrained_model.pt\")\n",
    "print(\"✓ Saved pretrained model to tutorial_torch/pretrained_model.pt\")\n",
    "\n",
    "# Store pretrained weights for later comparison\n",
    "pretrained_weights = [w.clone() for w in collect_weight_params(model)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize pretrained weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_weight_histogram(\n",
    "    weights=pretrained_weights,\n",
    "    title=\"Pretrained Weight Distribution\",\n",
    "    log_scale=False,\n",
    "    save=\"tutorial_torch/pretrained_histogram.png\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: Retrain with Gaussian Mixture Prior\n",
    "\n",
    "Now we add a **Gaussian mixture prior** over the weights:\n",
    "\n",
    "$$p(w) = \\sum_{j=0}^{J-1} \\pi_j \\mathcal{N}(w | \\mu_j, \\sigma_j^2)$$\n",
    "\n",
    "Where:\n",
    "- **J = 16 components** (1 zero-spike + 15 non-zero clusters)\n",
    "- **π₀ = 0.99** (high probability on zero for sparsity)\n",
    "- **μ₀ = 0** (zero component is fixed)\n",
    "- **μ₁...μ₁₅** are learned from the pretrained weight distribution\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "The training loss becomes:\n",
    "\n",
    "$$\\mathcal{L} = \\text{CrossEntropy}(y, \\hat{y}) + \\frac{\\tau}{N} \\sum_i -\\log p(w_i)$$\n",
    "\n",
    "Where:\n",
    "- **τ = 0.003** (complexity regularization strength)\n",
    "- **N = 60,000** (dataset size for proper normalization)\n",
    "\n",
    "The negative log probability term encourages weights to cluster at the mixture component means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Gaussian Mixture Prior"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize mixture with 16 components\n",
    "prior = init_mixture(\n",
    "    model=model,\n",
    "    J=16,  # Total components (1 zero + 15 non-zero)\n",
    "    pi0=0.99,  # High probability on zero component\n",
    "    init_means_mode=\"from_weights\",  # Initialize from pretrained weight range\n",
    "    init_sigma=0.25,  # Initial standard deviation\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Initialized Gaussian mixture prior\")\n",
    "print(f\"  - Number of components: {prior.J}\")\n",
    "print(f\"  - Zero component mixing weight (π₀): {prior.pi0_init}\")\n",
    "\n",
    "# Visualize initial mixture\n",
    "mu, sigma2, pi = prior.mixture_params()\n",
    "print(f\"\\nInitial mixture means: {mu[:5].detach().cpu().numpy()}...\")\n",
    "print(f\"Initial mixture stds: {torch.sqrt(sigma2[:5]).detach().cpu().numpy()}...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Setup GIF Visualizer\n\nWe'll use the `TrainingGifVisualizer` to create an animated visualization showing how weights evolve during retraining. The GIF will show:\n- Weight scatter plot (pretrained vs current weights)\n- Marginal histograms\n- Mixture component bands (mean ± 2σ)\n- Test accuracy per epoch",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create GIF visualizer to track weight evolution during retraining\n",
    "viz = TrainingGifVisualizer(\n",
    "    out_dir=\"tutorial_torch\",\n",
    "    tag=\"retraining\",\n",
    "    framerate=2,\n",
    "    notebook_display = True,\n",
    "    cleanup_frames = True\n",
    ")\n",
    "\n",
    "print(\"✓ GIF visualizer ready - will capture frames during training\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Retrain with Soft Weight-Sharing"
  },
  {
   "cell_type": "code",
   "source": [
    "# Retrain with mixture prior and GIF visualization\n",
    "retrain_acc = retrain_soft_weight_sharing(\n",
    "    model=model,\n",
    "    prior=prior,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    lr_w=5e-4,  # Learning rate for network weights (matches Keras)\n",
    "    lr_theta_means=1e-4,  # Learning rate for mixture means (slower, allows gradual shift)\n",
    "    lr_theta_gammas=3e-3,  # Learning rate for variances (FAST - critical for clustering!)\n",
    "    lr_theta_rhos=3e-3,  # Learning rate for mixing proportions\n",
    "    weight_decay=0.0,\n",
    "    tau=0.003,  # Complexity regularization (properly normalized by dataset size)\n",
    "    tau_warmup_epochs=0,  # Gradually increase tau over first 10 epochs\n",
    "    complexity_mode=\"keras\",  # Use keras-style normalization (tau/dataset_size)\n",
    "    eval_every=10,\n",
    "    cr_every=0,  # Don't compute compression during training (slow)\n",
    "    mixture_every=1,  # Log mixture every epoch for dynamics plot\n",
    "    run_dir=\"tutorial_torch\",\n",
    "    viz=viz  # Pass visualizer to capture frames during training\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Retrained model accuracy: {retrain_acc:.4f} ({retrain_acc*100:.2f}%)\")\n",
    "print(f\"  Accuracy drop from pretraining: {(pretrain_acc - retrain_acc)*100:.2f}%\")\n",
    "print(f\"\\n✓ GIF animation saved to: tutorial_torch/retraining.gif\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### View the Training Animation\n\nThe GIF shows how weights migrate from their pretrained values (x-axis) toward mixture component means (y-axis) during retraining. You'll see:\n- Weights clustering into horizontal bands (the mixture components)\n- The marginal histogram becoming more sparse and discrete\n- Test accuracy tracked in the title",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the GIF inline (works in Jupyter Lab/Notebook)\n",
    "from IPython.display import Image, display\n",
    "\n",
    "gif_path = \"tutorial_torch/retraining.gif\"\n",
    "if os.path.exists(gif_path):\n",
    "    display(Image(filename=gif_path))\n",
    "    print(f\"\\n💡 Animation also saved as individual frames in: tutorial_torch/retraining_frames/\")\n",
    "else:\n",
    "    print(f\"GIF not found at {gif_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save retrained (pre-quantized) model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save pre-quantized model\n",
    "torch.save(model.state_dict(), \"tutorial_torch/retrained_prequant_model.pt\")\n",
    "print(\"✓ Saved retrained model to tutorial_torch/retrained_prequant_model.pt\")\n",
    "\n",
    "# Store retrained weights for comparison\n",
    "retrained_weights = [w.clone() for w in collect_weight_params(model)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize learned mixture components"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the learned Gaussian mixture\n",
    "plot_mixture_components(\n",
    "    prior=prior,\n",
    "    xlim=(-0.5, 0.5),\n",
    "    save=\"tutorial_torch/learned_mixture.png\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot how mixture components evolved during retraining\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"scripts/plot_mixture_dynamics.py\", \"--run-dir\", \"tutorial_torch\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(result.stdout)\n",
    "    # Display the plot\n",
    "    dynamics_path = \"tutorial_torch/plot_mixture_dynamics.png\"\n",
    "    if os.path.exists(dynamics_path):\n",
    "        from IPython.display import Image, display\n",
    "        display(Image(filename=dynamics_path))\n",
    "        print(\"\\n💡 This plot shows how mixture means (±2σ) evolved during retraining\")\n",
    "else:\n",
    "    print(\"Error running plot script:\")\n",
    "    print(result.stderr)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Visualize mixture dynamics over training",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize weight movement"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Scatter plot showing how weights moved during retraining\n",
    "plot_weight_scatter(\n",
    "    weights_before=pretrained_weights,\n",
    "    weights_after=retrained_weights,\n",
    "    sample=20000,\n",
    "    xlim=(-0.5, 0.5),\n",
    "    ylim=(-0.5, 0.5),\n",
    "    save=\"tutorial_torch/weight_movement.png\"\n",
    ")\n",
    "\n",
    "print(\"\\n💡 Weights should cluster towards mixture component means (away from diagonal)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: Post-Processing and Compression\n",
    "\n",
    "Now we:\n",
    "1. **Quantize** weights by assigning each to its nearest mixture mean\n",
    "2. **Evaluate** the quantized model accuracy\n",
    "3. **Compute compression** using CSR + Huffman encoding\n",
    "\n",
    "## Quantization Strategy\n",
    "\n",
    "We use **maximum likelihood (ML) assignment**:\n",
    "- For each weight, find the component with highest likelihood\n",
    "- Replace weight with that component's mean\n",
    "- This avoids bias towards the zero-spike during snapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize weights to mixture means"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Quantize: snap each weight to its nearest mixture component mean\n",
    "prior.quantize_model(\n",
    "    model=model,\n",
    "    skip_last_matrix=True,  # Keep final classifier at full precision\n",
    "    assign=\"ml\"  # Use maximum likelihood assignment (avoids zero-spike bias)\n",
    ")\n",
    "\n",
    "print(\"✓ Quantized weights to mixture means\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate quantized model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate accuracy after quantization\n",
    "quantized_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "print(f\"\\n📊 Accuracy Comparison:\")\n",
    "print(f\"  Pretrained:  {pretrain_acc:.4f} ({pretrain_acc*100:.2f}%)\")\n",
    "print(f\"  Retrained:   {retrain_acc:.4f} ({retrain_acc*100:.2f}%)\")\n",
    "print(f\"  Quantized:   {quantized_acc:.4f} ({quantized_acc*100:.2f}%)\")\n",
    "print(f\"\\n  Total accuracy drop: {(pretrain_acc - quantized_acc)*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save quantized model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save final quantized model\n",
    "torch.save(model.state_dict(), \"tutorial_torch/quantized_model.pt\")\n",
    "print(\"✓ Saved quantized model to tutorial_torch/quantized_model.pt\")\n",
    "\n",
    "# Store quantized weights\n",
    "quantized_weights = [w.clone() for w in collect_weight_params(model)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute compression statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Detailed compression report using CSR + Huffman encoding\n",
    "report = compression_report(\n",
    "    model=model,\n",
    "    prior=prior,\n",
    "    dataset=\"mnist\",\n",
    "    use_huffman=True,\n",
    "    pbits_fc=5,  # Bits for FC layer column index diffs\n",
    "    pbits_conv=8,  # Bits for Conv layer column index diffs\n",
    "    skip_last_matrix=True,  # Last layer was not quantized\n",
    "    assign_mode=\"ml\"  # Must match quantization assignment mode\n",
    ")\n",
    "\n",
    "print(\"\\n Compression Report:\")\n",
    "print(f\"  Original bits:    {report['orig_bits']:,}\")\n",
    "print(f\"  Compressed bits:  {report['compressed_bits']:,}\")\n",
    "print(f\"  Compression Ratio: {report['CR']:.2f}x\")\n",
    "print(f\"  Non-zero weights:  {report['nnz']:,} / {weight_params:,} ({100*report['nnz']/weight_params:.2f}%)\")\n",
    "print(f\"  Sparsity:         {100*(1 - report['nnz']/weight_params):.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise compression breakdown"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n📋 Layer-wise Compression:\")\n",
    "print(f\"{'Layer':<15} {'Shape':<20} {'Original (bits)':<18} {'Compressed (bits)':<20} {'CR':<8} {'Sparsity':<10}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for layer_info in report['layers']:\n",
    "    if layer_info['passthrough']:\n",
    "        cr_str = \"N/A\"\n",
    "        sparsity = 0.0\n",
    "    else:\n",
    "        compressed = layer_info['bits_IR'] + layer_info['bits_IC'] + layer_info['bits_A'] + layer_info['bits_codebook']\n",
    "        cr = layer_info['orig_bits'] / max(compressed, 1)\n",
    "        cr_str = f\"{cr:.2f}x\"\n",
    "        total_weights = np.prod(layer_info['shape'])\n",
    "        sparsity = 100 * (1 - layer_info['nnz'] / total_weights)\n",
    "    \n",
    "    shape_str = 'x'.join(map(str, layer_info['shape']))\n",
    "    orig_str = f\"{layer_info['orig_bits']:,}\"\n",
    "    comp_str = f\"{layer_info['bits_IR'] + layer_info['bits_IC'] + layer_info['bits_A'] + layer_info['bits_codebook']:,}\"\n",
    "    \n",
    "    print(f\"{layer_info['layer']:<15} {shape_str:<20} {orig_str:<18} {comp_str:<20} {cr_str:<8} {sparsity:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize weight distributions across all phases"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare histograms: pretrained → retrained → quantized\n",
    "plot_comparison_histograms(\n",
    "    weights_pre=pretrained_weights,\n",
    "    weights_retrained=retrained_weights,\n",
    "    weights_quantized=quantized_weights,\n",
    "    save_prefix=\"tutorial_torch/phase\"\n",
    ")\n",
    "\n",
    "print(\"\\n💡 Notice how weights progressively cluster and become sparse\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "We successfully applied soft weight-sharing to compress a neural network:\n",
    "\n",
    "### Results:\n",
    "- **Original model**: ~642K parameters at 32-bit → ~20.5 MB\n",
    "- **Compressed model**: Achieves ~20-60x compression (depending on hyperparameters)\n",
    "- **Accuracy loss**: Typically < 1% on MNIST\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Soft clustering is better than hard**: By using a differentiable mixture prior, weights naturally migrate to cluster centers during training\n",
    "\n",
    "2. **Zero-spike induces sparsity**: The high mixing weight (π₀ = 0.99) on the zero component encourages many weights to become exactly zero\n",
    "\n",
    "3. **Proper normalization matters**: The complexity term must be normalized by dataset size (τ/N) for stable training\n",
    "\n",
    "4. **CSR + Huffman is efficient**: Sparse weights + entropy coding achieves better compression than naive approaches\n",
    "\n",
    "### Next Steps:\n",
    "- Try different number of components (J)\n",
    "- Experiment with different τ values\n",
    "- Apply to larger models (ResNets, etc.)\n",
    "- Combine with other compression techniques (pruning, quantization-aware training)\n",
    "\n",
    "---\n",
    "\n",
    "**Reference**: Ullrich, K., Meeds, E., & Welling, M. (2017). Soft Weight-Sharing for Neural Network Compression. *ICLR 2017*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Analyze Mixture Component Usage"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Count how many weights are assigned to each component\nmu, sigma2, pi = prior.mixture_params()\nmu = mu.detach().cpu().numpy()\npi = pi.detach().cpu().numpy()\n\n# Count assignments in quantized model\nall_weights = torch.cat([w.flatten() for w in quantized_weights]).cpu().detach().numpy()\n\nprint(\"\\n📊 Component Assignment Statistics:\\n\")\nprint(f\"{'Component':<12} {'Mean (μ)':<12} {'Mixing (π)':<12} {'# Weights':<15} {'Percentage':<12}\")\nprint(\"-\" * 70)\n\nfor j in range(len(mu)):\n    # Count weights at this mean (with small tolerance for floating point)\n    count = np.sum(np.abs(all_weights - mu[j]) < 1e-6)\n    percentage = 100 * count / len(all_weights)\n    \n    comp_name = f\"Comp {j}\" if j > 0 else \"Zero-spike\"\n    print(f\"{comp_name:<12} {mu[j]:>11.4f} {pi[j]:>11.4f} {count:>14,} {percentage:>11.2f}%\")\n\nprint(\"\\n💡 The zero-spike should capture most weights (high sparsity)\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
